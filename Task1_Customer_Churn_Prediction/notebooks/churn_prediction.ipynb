{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Churn Prediction\n",
        "This notebook demonstrates a simple churn prediction pipeline using a public Telco dataset.\n",
        "It downloads the dataset from a public raw GitHub URL when run locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from src.data_preprocessing import load_data_from_url, basic_cleaning\n",
        "from src.model_training import train_logistic_regression, train_random_forest, evaluate_model, save_model\n",
        "from src.visualization import plot_churn_distribution, plot_feature_importance\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Download dataset\n",
        "The notebook will download the dataset from a public raw GitHub URL. If you prefer, you can put the CSV into `data/` folder and skip the download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Public Telco dataset raw URL (IBM sample on GitHub)\n",
        "DATA_URL = 'https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv'\n",
        "data_path = 'data/Telco-Customer-Churn.csv'\n",
        "os.makedirs('data', exist_ok=True)\n",
        "try:\n",
        "    df = load_data_from_url(DATA_URL, save_path=data_path)\n",
        "    print('Dataset downloaded to', data_path)\n",
        "except Exception as e:\n",
        "    print('Failed to download dataset automatically. Please download manually and place in data/ folder.', e)\n",
        "    if os.path.exists(data_path):\n",
        "        df = pd.read_csv(data_path)\n",
        "    else:\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic cleaning\n",
        "df = basic_cleaning(df)\n",
        "if 'Churn' in df.columns:\n",
        "    df['Churn'] = df['Churn'].map({'Yes':1, 'No':0})\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick EDA: churn distribution\n",
        "plot_churn_distribution(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data: simple encoding\n",
        "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "df_enc = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "X = df_enc.drop('Churn', axis=1)\n",
        "y = df_enc['Churn']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train models\n",
        "log_model = train_logistic_regression(X_train_scaled, y_train)\n",
        "rf_model = train_random_forest(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "print('Logistic Regression:')\n",
        "print(evaluate_model(log_model, X_test_scaled, y_test))\n",
        "print('\\nRandom Forest:')\n",
        "print(evaluate_model(rf_model, X_test, y_test))\n",
        "\n",
        "# Save models\n",
        "os.makedirs('models', exist_ok=True)\n",
        "save_model(rf_model, 'models/rf_model.joblib')\n",
        "save_model(log_model, 'models/log_model.joblib')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (RF)\n",
        "try:\n",
        "    import numpy as np\n",
        "    feats = X.columns.tolist()\n",
        "    imps = rf_model.feature_importances_\n",
        "    plot_feature_importance(feats, imps, top_n=20, save_path='models/feature_importance.png')\n",
        "except Exception as e:\n",
        "    print('Could not plot feature importance.', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "- This notebook provides a baseline churn prediction pipeline. Improve by hyperparameter tuning, feature engineering, and using cross-validation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}